---
title: Tanzu Observability and TAS Troubleshooting
keywords: best practices
tags:
sidebar: doc_sidebar
permalink: tas_to_troubleshooting.html
summary: Solve problems with the Tanzu Observability tile and the TAS integration.
---

This doc page looks at possible causes for problems you might encounter with your Tanzu Application Service (TAS) to Tanzu Observability integration and explains how to address them.

## Sizing and Scaling for Large TAS Foundations

Larger TAS foundations are more demanding to monitor than smaller foundations.
* If more application instances (AIs) are running on a foundation, then more container-level metrics have to be collected and forwarde to Wavefront.
* Similarly, if more virtual machines are in a foundation, then more VM-level metrics are reported.

If your foundation is large, tune the following parameters, in this order:
1. Increase the size of your **Telegraf Agent Virtual Machine**. The Telegraf agent is responsible for collecting metrics and transforming them into the Wavefront output format. It is typically CPU and memory bound, so increasing virtual machine size can increase perfrmance.
2. **Reduce the scrape interval**. If you see that collection times for some of your scrape targets are greater than 12 seconds, consider changing the scrape interval for your environment to a lower frequency. Typically, 120% of the longest observed collection time should be safe.

## No Data Flowing into Tanzu Observability

**Symptom**: You have successfully set up the nozzle and the integration. However, you don't see any data for the out-of-the-box dashboards.

**Potential Solutions**:
* Ensure that the setup flow has completed. Check back a few hours after you perform setup.
* Verify that the proxy uses the correct API token and Wavefront instance URL. You see that information during nozzle setup in the XX page.
* Verify that the TO environment has connectivity to the wavefront host
* In your Tanzu Application Service environment, verify that the Bosh jobs for Wavefront proxy and for the Telegraf agent are running

## Dashboards Do Not Show Any Data

The most common cause for the TAS dashboards to not show any data is a problem with sending data to Tanzu Observability. See Symptom: No Data Flowing into Tanzu Observability.

## Higher than Expected PPS Rate

The PPS (points-per-second) rate can affect performance and potentially the cost of using Tanzu Observability. In general, the PPS generated by the TAS Nozzle version 4.x should be predictable and relatively consistent for any given foundation, because metrics are scraped at a fixed interval. In contrast, version 3.x of the Nozzle followed a push-based model, and PPS varied based on factors such as HTTP requests being served by the gorouter.

However, it can be difficult to predict the average PPS of a TAS foundation ahead of time because several factors affect the total number of metrics that are generated:
* The TAS version
* The size of the foundation
* Other TAS components running on the foundation

PPS might increase or decrease when individual TAS components are installed, upgraded or removed. Each individual component contributes its own metrics.

**Solution**:

* Increase the Telegraf agentâ€™s scrape interval. Metrics will be collected less frequently, and average PPS will fall.

Future releases will allow more targeted approaches to reducing PPS, for example, by filtering out unwanted metrics.


## Incomplete Data in Tanzu Observability

**Symptom**:

Data from your TAS foundation are visible in Tanzu Observability dashboards and charts, but seem incomplete.

**Potential Cause**:

Incomplete data is most likely caused by one or more components failing to keep up with the volume of metrics being generated by the platform. Typically this happens when the gauge exporter emits large numbers of metrics, and the Telegraf agent is not able to ingest these and forward them to the Wavefront Proxy before the next collection cycle begins. In those cases, errors are more likely to occur and metrics will be dropped as the agent tries to catch up.

**Investigation**:

Here are some things you can do.
* Look for errors in bpm logs on the telegraf agent or in the Wavefront Proxy logs. See [Proxy Troubleshooting](proxies_troubleshooting.html) and [Telegraf Troubleshooting](telegraf_details.html) for details.
* Look for collection errors from Telegraf (`tas.observability.telegraf.internal_gather.errors`)
* Look for long collection times from Telegraf (`tas.observability.telegraf.internal_gather.gather_time_ns`)

**Potential Solutions**:

Increase the size of the Telegraf Agent Virtual Machine
Increase the Scrape Interval
